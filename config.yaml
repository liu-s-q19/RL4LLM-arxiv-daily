# TODO: add papers by configuration file
base_url: "https://arxiv.paperswithcode.com/api/v0/papers/"
# =========================================================
# 请确保这里是您的用户名和仓库名
# =========================================================
user_name: "liu-s-q19"
repo_name: "RL4LLM-arxiv-daily"

show_authors: True
show_links: True
show_badge: True
max_results: 15

publish_readme: True
publish_gitpage: True
publish_wechat: False

# file paths
json_readme_path: './docs/cv-arxiv-daily.json'
json_gitpage_path: './docs/cv-arxiv-daily-web.json'
json_wechat_path: './docs/cv-arxiv-daily-wechat.json'

md_readme_path: 'README.md'
md_gitpage_path: './docs/index.md'
md_wechat_path: './docs/wechat.md'

# keywords to search
# 修改说明：使用了 - 符号列表格式，彻底解决 parsing error
keywords:
  "LLM RL Algorithms":
    filters:
      - "Group Relative Policy Optimization"
      - "GRPO"
      - "DAPO"
      - "GSPO"
      - "RLVR"
  # === 2. RL + LLM 的各种组合 (考虑缩写 RL 和全称) ===
      # 基准: Reinforcement Learning with LLMs
      - "Reinforcement Learning with LLM"
      - "Reinforcement Learning for LLM"
      - "RL with LLM"
      - "RL for LLM"
      - "LLM Reinforcement Learning"
      - "Large Language Model Reinforcement Learning"
      
      # === 3. RL + Reasoning 的各种组合 (推理能力的 RL) ===
      # 基准: Reinforcement Learning for Reasoning Language Models / LLM Reasoning
      - "Reinforcement Learning for Reasoning"
      - "RL for Reasoning"
      - "Reasoning with Reinforcement Learning"
      - "Reasoning with RL"
      - "LLM Reasoning with RL"

# 逻辑：必须体现 "RL/Reward" 与 "Reasoning/CoT" 的结合
  "Reward & RLVR":
    filters:
      # --- 核心机制 (Process Reward 是 Reasoning RL 的标志) ---
      - "Process Reward Model"
      - "PRM for Reasoning"
      - "Verifiable Reward"
      - "Outcome Reward"
      
      # --- CoT + RL 的组合 (防止抓到单纯的 Prompting) ---
      - "Reinforcement Learning for Chain of Thought"
      - "RL for Chain of Thought"
      - "RL for CoT"
      - "Chain of Thought Optimization"
      
      # --- Self-Correction + RL 的组合 ---
      - "Self-Correction with Reinforcement Learning"
      - "Self-Correction with RL"
      - "RL based Self-Correction"
      
      # --- System 2 + RL ---
      - "System 2 Reasoning with RL"
      - "System 2 RL"

  # 逻辑：必须体现 "LLM/VLM/RL" 与 "Driving/Embodied" 的结合
  # 防止抓到：传统的 PID 控制、纯视觉检测、不含 AI 的机械控制
  "RL & LLM for Auto-Driving":
    filters:
      # --- VLA / VLM 结合驾驶 ---
      - "Vision Language Action for Driving"
      - "VLA for Autonomous Driving"
      - "VLM for Autonomous Driving"
      - "Vision Language Model for Driving"
      
      # --- LLM 结合驾驶 ---
      - "LLM for Autonomous Driving"
      - "Large Language Model for Autonomous Driving"
      - "Language-guided Driving Policy"
      - "Language-conditioned Driving"
      
      # --- RL 结合驾驶 (端到端/世界模型) ---
      - "Reinforcement Learning for Autonomous Driving"
      - "RL for Autonomous Driving"
      - "End-to-end RL for Driving"
