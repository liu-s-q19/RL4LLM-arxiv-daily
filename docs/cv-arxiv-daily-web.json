{"LLM RL Algorithms": {"2512.16912": "|**2025-12-21**|**Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward**|Peter Chen et.al.|[2512.16912](http://arxiv.org/abs/2512.16912)|null|\n", "2512.15687": "|**2025-12-17**|**Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning**|Zhenwen Liang et.al.|[2512.15687](http://arxiv.org/abs/2512.15687)|null|\n", "2512.15274": "|**2025-12-17**|**Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning**|Yiliu Sun et.al.|[2512.15274](http://arxiv.org/abs/2512.15274)|null|\n", "2512.15146": "|**2025-12-18**|**Beyond Majority Voting: Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning**|Weiqin Wang et.al.|[2512.15146](http://arxiv.org/abs/2512.15146)|null|\n", "2512.14944": "|**2025-12-16**|**Puzzle Curriculum GRPO for Vision-Centric Reasoning**|Ahmadreza Jeddi et.al.|[2512.14944](http://arxiv.org/abs/2512.14944)|null|\n", "2512.14698": "|**2025-12-16**|**TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs**|Jun Zhang et.al.|[2512.14698](http://arxiv.org/abs/2512.14698)|null|\n", "2512.13668": "|**2025-12-15**|**A Scientific Reasoning Model for Organic Synthesis Procedure Generation**|Guoqing Liu et.al.|[2512.13668](http://arxiv.org/abs/2512.13668)|null|\n", "2512.13607": "|**2025-12-15**|**Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models**|Boxin Wang et.al.|[2512.13607](http://arxiv.org/abs/2512.13607)|null|\n", "2512.13106": "|**2025-12-15**|**TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning**|Shenzhi Yang et.al.|[2512.13106](http://arxiv.org/abs/2512.13106)|null|\n", "2512.12487": "|**2025-12-13**|**More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models**|Hoang Anh Just et.al.|[2512.12487](http://arxiv.org/abs/2512.12487)|null|\n", "2512.12476": "|**2025-12-13**|**HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments**|Yongjun He et.al.|[2512.12476](http://arxiv.org/abs/2512.12476)|null|\n", "2512.10756": "|**2025-12-11**|**OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification**|Zijian Wu et.al.|[2512.10756](http://arxiv.org/abs/2512.10756)|null|\n", "2512.10739": "|**2025-12-12**|**Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving**|Songyang Gao et.al.|[2512.10739](http://arxiv.org/abs/2512.10739)|null|\n", "2512.07611": "|**2025-12-08**|**Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement**|Yongsheng Lian et.al.|[2512.07611](http://arxiv.org/abs/2512.07611)|null|\n", "2512.07558": "|**2025-12-08**|**ReLaX: Reasoning with Latent Exploration for Large Reasoning Models**|Shimin Zhang et.al.|[2512.07558](http://arxiv.org/abs/2512.07558)|null|\n", "2512.13874": "|**2025-12-15**|**SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning**|Jitesh Jain et.al.|[2512.13874](http://arxiv.org/abs/2512.13874)|null|\n", "2512.12922": "|**2025-12-15**|**LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization**|Bangyu Li et.al.|[2512.12922](http://arxiv.org/abs/2512.12922)|null|\n", "2512.12888": "|**2025-12-15**|**Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence**|David Dang et.al.|[2512.12888](http://arxiv.org/abs/2512.12888)|null|\n", "2512.10949": "|**2025-12-11**|**Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation**|Yiwen Tang et.al.|[2512.10949](http://arxiv.org/abs/2512.10949)|null|\n", "2512.09487": "|**2025-12-10**|**RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning**|Yucan Guo et.al.|[2512.09487](http://arxiv.org/abs/2512.09487)|null|\n", "2512.08889": "|**2025-12-09**|**No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers**|Damiano Marsili et.al.|[2512.08889](http://arxiv.org/abs/2512.08889)|null|\n", "2512.07783": "|**2025-12-08**|**On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models**|Charlie Zhang et.al.|[2512.07783](http://arxiv.org/abs/2512.07783)|null|\n", "2512.06835": "|**2025-12-07**|**Decouple to Generalize: Context-First Self-Evolving Learning for Data-Scarce Vision-Language Reasoning**|Tingyu Li et.al.|[2512.06835](http://arxiv.org/abs/2512.06835)|null|\n", "2512.03783": "|**2025-12-04**|**Omni-AutoThink: Adaptive Multimodal Reasoning via Reinforcement Learning**|Dongchao Yang et.al.|[2512.03783](http://arxiv.org/abs/2512.03783)|null|\n", "2512.01485": "|**2025-12-08**|**Multi-Path Collaborative Reasoning via Reinforcement Learning**|Jindi Lv et.al.|[2512.01485](http://arxiv.org/abs/2512.01485)|null|\n", "2512.00831": "|**2025-12-09**|**ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning**|Yuchen Zeng et.al.|[2512.00831](http://arxiv.org/abs/2512.00831)|null|\n", "2512.00499": "|**2025-11-29**|**ESPO: Entropy Importance Sampling Policy Optimization**|Yuepeng Sheng et.al.|[2512.00499](http://arxiv.org/abs/2512.00499)|null|\n", "2511.23477": "|**2025-11-28**|**Video-CoM: Interactive Video Reasoning via Chain of Manipulations**|Hanoona Rasheed et.al.|[2511.23477](http://arxiv.org/abs/2511.23477)|null|\n", "2511.22570": "|**2025-11-27**|**DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning**|Zhihong Shao et.al.|[2511.22570](http://arxiv.org/abs/2511.22570)|null|\n", "2510.22977": "|**2025-10-27**|**The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination**|Chenlong Yin et.al.|[2510.22977](http://arxiv.org/abs/2510.22977)|null|\n", "2510.21122": "|**2025-10-29**|**NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation**|Longtian Qiu et.al.|[2510.21122](http://arxiv.org/abs/2510.21122)|null|\n", "2510.18176": "|**2025-10-20**|**Local Coherence or Global Validity? Investigating RLVR Traces in Math Domains**|Soumya Rani Samineni et.al.|[2510.18176](http://arxiv.org/abs/2510.18176)|null|\n", "2510.15502": "|**2025-10-17**|**The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling**|Shijia Kang et.al.|[2510.15502](http://arxiv.org/abs/2510.15502)|null|\n", "2510.08233": "|**2025-10-09**|**Enhancing Reasoning for Diffusion LLMs via Distribution Matching Policy Optimization**|Yuchen Zhu et.al.|[2510.08233](http://arxiv.org/abs/2510.08233)|null|\n", "2510.05194": "|**2025-10-06**|**Reinforcement Learning for Clinical Reasoning: Aligning LLMs with ACR Imaging Appropriateness Criteria**|Anni Tziakouri et.al.|[2510.05194](http://arxiv.org/abs/2510.05194)|null|\n", "2510.01135": "|**2025-10-01**|**Prompt Curriculum Learning for Efficient LLM Post-Training**|Zhaolin Gao et.al.|[2510.01135](http://arxiv.org/abs/2510.01135)|null|\n", "2509.25026": "|**2025-10-14**|**GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning**|Mustansar Fiaz et.al.|[2509.25026](http://arxiv.org/abs/2509.25026)|null|\n", "2509.21880": "|**2025-12-27**|**No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping**|Thanh-Long V. Le et.al.|[2509.21880](http://arxiv.org/abs/2509.21880)|null|\n", "2509.09082": "|**2025-09-11**|**MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction**|Zhongqiu Li et.al.|[2509.09082](http://arxiv.org/abs/2509.09082)|null|\n", "2508.14765": "|**2025-11-20**|**PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning**|Ruheng Wang et.al.|[2508.14765](http://arxiv.org/abs/2508.14765)|null|\n", "2508.07616": "|**2025-08-21**|**ThinkTuning: Instilling Cognitive Reflections without Distillation**|Aswin RRV et.al.|[2508.07616](http://arxiv.org/abs/2508.07616)|null|\n", "2507.13142": "|**2025-09-26**|**From Roots to Rewards: Dynamic Tree Reasoning with Reinforcement Learning**|Ahmed Bahloul et.al.|[2507.13142](http://arxiv.org/abs/2507.13142)|null|\n", "2507.06167": "|**2025-07-10**|**Skywork-R1V3 Technical Report**|Wei Shen et.al.|[2507.06167](http://arxiv.org/abs/2507.06167)|null|\n", "2506.07905": "|**2025-06-09**|**WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning**|Jie Yang et.al.|[2506.07905](http://arxiv.org/abs/2506.07905)|null|\n", "2512.17043": "|**2025-12-18**|**UniRel-R1: RL-tuned LLM Reasoning for Knowledge Graph Relational Question Answering**|Yinxu Tang et.al.|[2512.17043](http://arxiv.org/abs/2512.17043)|null|\n", "2512.17206": "|**2025-12-19**|**Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs**|Rujiao Long et.al.|[2512.17206](http://arxiv.org/abs/2512.17206)|null|\n", "2512.19554": "|**2025-12-22**|**CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal**|Yongxin Wang et.al.|[2512.19554](http://arxiv.org/abs/2512.19554)|null|\n", "2512.18857": "|**2025-12-21**|**CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning**|Zijun Gao et.al.|[2512.18857](http://arxiv.org/abs/2512.18857)|null|\n", "2512.18730": "|**2025-12-21**|**A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models**|Zhiquan Tan et.al.|[2512.18730](http://arxiv.org/abs/2512.18730)|null|\n", "2512.18623": "|**2025-12-21**|**LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction**|Jensen Zhang et.al.|[2512.18623](http://arxiv.org/abs/2512.18623)|null|\n", "2512.18215": "|**2025-12-20**|**Stable and Efficient Single-Rollout RL for Multimodal Reasoning**|Rui Liu et.al.|[2512.18215](http://arxiv.org/abs/2512.18215)|null|\n", "2512.19920": "|**2025-12-25**|**Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning**|Jiayun Wu et.al.|[2512.19920](http://arxiv.org/abs/2512.19920)|null|\n", "2512.20312": "|**2025-12-25**|**TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning**|Saisai Yang et.al.|[2512.20312](http://arxiv.org/abs/2512.20312)|null|\n", "2512.20760": "|**2025-12-23**|**Generalization of RLVR Using Causal Reasoning as a Testbed**|Brian Lu et.al.|[2512.20760](http://arxiv.org/abs/2512.20760)|null|\n", "2512.20856": "|**2025-12-24**|**NVIDIA Nemotron 3: Efficient and Open Intelligence**|NVIDIA et.al.|[2512.20856](http://arxiv.org/abs/2512.20856)|null|\n", "2512.21625": "|**2025-12-25**|**Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards**|Xinyu Tang et.al.|[2512.21625](http://arxiv.org/abs/2512.21625)|null|\n", "2512.23165": "|**2025-12-30**|**Evaluating Parameter Efficient Methods for RLVR**|Qingyu Yin et.al.|[2512.23165](http://arxiv.org/abs/2512.23165)|null|\n", "2512.23515": "|**2025-12-29**|**Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning**|Zuoyou Jiang et.al.|[2512.23515](http://arxiv.org/abs/2512.23515)|null|\n", "2512.23087": "|**2025-12-28**|**Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning**|Yingru Li et.al.|[2512.23087](http://arxiv.org/abs/2512.23087)|null|\n", "2512.23075": "|**2025-12-28**|**Trust Region Masking for Long-Horizon LLM Reinforcement Learning**|Yingru Li et.al.|[2512.23075](http://arxiv.org/abs/2512.23075)|null|\n", "2512.22183": "|**2025-12-19**|**Unbiased Visual Reasoning with Controlled Visual Inputs**|Zhaonan Li et.al.|[2512.22183](http://arxiv.org/abs/2512.22183)|null|\n", "2512.25063": "|**2025-12-31**|**Many Minds from One Model: Bayesian Transformers for Population Intelligence**|Diji Yang et.al.|[2512.25063](http://arxiv.org/abs/2512.25063)|null|\n", "2512.24609": "|**2025-12-31**|**Reinforcement Learning-Augmented LLM Agents for Collaborative Decision Making and Performance Optimization**|Dong Qiu et.al.|[2512.24609](http://arxiv.org/abs/2512.24609)|null|\n", "2512.22716": "|**2025-12-31**|**Memento 2: Learning by Stateful Reflective Memory**|Jun Wang et.al.|[2512.22716](http://arxiv.org/abs/2512.22716)|null|\n", "2601.02036": "|**2026-01-05**|**GDRO: Group-level Reward Post-training Suitable for Diffusion Models**|Yiyang Wang et.al.|[2601.02036](http://arxiv.org/abs/2601.02036)|null|\n", "2601.01580": "|**2026-01-04**|**The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs**|Zibo Zhao et.al.|[2601.01580](http://arxiv.org/abs/2601.01580)|null|\n", "2601.03205": "|**2026-01-06**|**UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward**|Yile Liu et.al.|[2601.03205](http://arxiv.org/abs/2601.03205)|null|\n", "2601.03111": "|**2026-01-06**|**One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling**|Yiyuan Li et.al.|[2601.03111](http://arxiv.org/abs/2601.03111)|null|\n", "2601.03823": "|**2026-01-07**|**Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning**|Fei Wu et.al.|[2601.03823](http://arxiv.org/abs/2601.03823)|null|\n", "2601.03723": "|**2026-01-07**|**ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization**|Shijie Zhang et.al.|[2601.03723](http://arxiv.org/abs/2601.03723)|null|\n", "2601.03320": "|**2026-01-06**|**Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning**|Yu Luo et.al.|[2601.03320](http://arxiv.org/abs/2601.03320)|null|\n", "2601.03467": "|**2026-01-09**|**ThinkRL-Edit: Thinking in Reinforcement Learning for Reasoning-Centric Image Editing**|Hengjia Li et.al.|[2601.03467](http://arxiv.org/abs/2601.03467)|null|\n", "2601.05053": "|**2026-01-08**|**Reinforced Efficient Reasoning via Semantically Diverse Exploration**|Ziqi Zhao et.al.|[2601.05053](http://arxiv.org/abs/2601.05053)|null|\n", "2601.04611": "|**2026-01-08**|**Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR**|Yihong Tang et.al.|[2601.04611](http://arxiv.org/abs/2601.04611)|null|\n", "2601.04537": "|**2026-01-08**|**Not All Steps are Informative: On the Linearity of LLMs' RLVR Training**|Tianle Wang et.al.|[2601.04537](http://arxiv.org/abs/2601.04537)|null|\n", "2601.04411": "|**2026-01-07**|**Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards**|Ali Rad et.al.|[2601.04411](http://arxiv.org/abs/2601.04411)|null|\n", "2601.05870": "|**2026-01-09**|**IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck**|Huilin Deng et.al.|[2601.05870](http://arxiv.org/abs/2601.05870)|null|\n", "2601.05787": "|**2026-01-09**|**From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation**|Zezhou Wang et.al.|[2601.05787](http://arxiv.org/abs/2601.05787)|null|\n", "2601.05607": "|**2026-01-09**|**Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR**|Zijun Min et.al.|[2601.05607](http://arxiv.org/abs/2601.05607)|null|\n", "2601.05459": "|**2026-01-09**|**Do LLMs Need Inherent Reasoning Before Reinforcement Learning? A Study in Korean Self-Correction**|Hongjin Kim et.al.|[2601.05459](http://arxiv.org/abs/2601.05459)|null|\n", "2601.07782": "|**2026-01-12**|**Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning**|Wei Fang et.al.|[2601.07782](http://arxiv.org/abs/2601.07782)|null|\n", "2601.07349": "|**2026-01-12**|**Reward Modeling from Natural Language Human Feedback**|Zongqi Wang et.al.|[2601.07349](http://arxiv.org/abs/2601.07349)|null|\n", "2601.07320": "|**2026-01-12**|**Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training**|Xue Gong et.al.|[2601.07320](http://arxiv.org/abs/2601.07320)|null|\n", "2601.06801": "|**2026-01-11**|**Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy**|Shujian Gao et.al.|[2601.06801](http://arxiv.org/abs/2601.06801)|null|\n", "2601.06677": "|**2026-01-10**|**Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget**|Zohaib Khan et.al.|[2601.06677](http://arxiv.org/abs/2601.06677)|null|\n", "2601.07036": "|**2026-01-11**|**Mid-Think: Training-Free Intermediate-Budget Reasoning via Token-Level Triggers**|Wang Yang et.al.|[2601.07036](http://arxiv.org/abs/2601.07036)|null|\n", "2601.06795": "|**2026-01-22**|**GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization with Enhanced Training Data Utilization for Sample-Constrained Reinforcement Learning**|Zhengqing Yan et.al.|[2601.06795](http://arxiv.org/abs/2601.06795)|null|\n", "2601.08521": "|**2026-01-22**|**Your Group-Relative Advantage Is Biased**|Fengkai Yang et.al.|[2601.08521](http://arxiv.org/abs/2601.08521)|null|\n", "2601.08468": "|**2026-01-13**|**JudgeRLVR: Judge First, Generate Second for Efficient Reasoning**|Jiangshan Duo et.al.|[2601.08468](http://arxiv.org/abs/2601.08468)|null|\n", "2601.08430": "|**2026-01-13**|**RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation**|Sunzhu Li et.al.|[2601.08430](http://arxiv.org/abs/2601.08430)|null|\n", "2601.08237": "|**2026-01-13**|**The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination**|Haoran Su et.al.|[2601.08237](http://arxiv.org/abs/2601.08237)|null|\n", "2601.08107": "|**2026-01-13**|**STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order**|Chengyang Gu et.al.|[2601.08107](http://arxiv.org/abs/2601.08107)|null|\n", "2601.09361": "|**2026-01-14**|**GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR**|Jiaying Zhang et.al.|[2601.09361](http://arxiv.org/abs/2601.09361)|null|\n", "2601.09667": "|**2026-01-15**|**Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning**|Zhiyuan Hu et.al.|[2601.09667](http://arxiv.org/abs/2601.09667)|null|\n", "2601.10079": "|**2026-01-15**|**Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts**|Sijia Luo et.al.|[2601.10079](http://arxiv.org/abs/2601.10079)|null|\n", "2601.11061": "|**2026-01-16**|**Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs**|Lecheng Yan et.al.|[2601.11061](http://arxiv.org/abs/2601.11061)|null|\n", "2601.14251": "|**2026-01-20**|**LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR**|Said Taghadouini et.al.|[2601.14251](http://arxiv.org/abs/2601.14251)|null|\n", "2601.13284": "|**2026-01-19**|**Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning**|Duygu Nur Yaldiz et.al.|[2601.13284](http://arxiv.org/abs/2601.13284)|null|\n", "2601.12995": "|**2026-01-19**|**Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models**|Runxuan Liu et.al.|[2601.12995](http://arxiv.org/abs/2601.12995)|null|\n", "2601.12465": "|**2026-01-18**|**Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping**|Miao Peng et.al.|[2601.12465](http://arxiv.org/abs/2601.12465)|null|\n", "2601.12186": "|**2026-01-17**|**Aletheia: What Makes RLVR For Code Verifiers Tick?**|Vatsal Venkatkrishna et.al.|[2601.12186](http://arxiv.org/abs/2601.12186)|null|\n", "2601.11864": "|**2026-01-17**|**AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training**|Zhiyuan Li et.al.|[2601.11864](http://arxiv.org/abs/2601.11864)|null|\n", "2601.13115": "|**2026-01-19**|**Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning**|Fengran Mo et.al.|[2601.13115](http://arxiv.org/abs/2601.13115)|null|\n", "2601.14711": "|**2026-01-21**|**DARA: Few-shot Budget Allocation in Online Advertising via In-Context Decision Making with RL-Finetuned LLMs**|Mingxuan Song et.al.|[2601.14711](http://arxiv.org/abs/2601.14711)|null|\n", "2601.14700": "|**2026-01-21**|**DARL: Encouraging Diverse Answers for General Reasoning without Verifiers**|Chongxuan Huang et.al.|[2601.14700](http://arxiv.org/abs/2601.14700)|null|\n", "2601.14686": "|**2026-01-21**|**IB-GRPO: Aligning LLM-based Learning Path Recommendation with Educational Objectives via Indicator-Based Group Relative Policy Optimization**|Shuai Wang et.al.|[2601.14686](http://arxiv.org/abs/2601.14686)|null|\n", "2601.15737": "|**2026-01-22**|**PhysProver: Advancing Automatic Theorem Proving for Physics**|Hanning Zhang et.al.|[2601.15737](http://arxiv.org/abs/2601.15737)|null|\n", "2601.15609": "|**2026-01-26**|**When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards**|Mingyuan Fan et.al.|[2601.15609](http://arxiv.org/abs/2601.15609)|null|\n", "2601.15330": "|**2026-01-20**|**ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation**|Zhebo Wang et.al.|[2601.15330](http://arxiv.org/abs/2601.15330)|null|\n", "2601.16853": "|**2026-01-23**|**Reasoning Promotes Robustness in Theory of Mind Tasks**|Ian B. de Haan et.al.|[2601.16853](http://arxiv.org/abs/2601.16853)|null|\n", "2601.16480": "|**2026-01-23**|**TL-GRPO: Turn-Level RL for Reasoning-Guided Iterative Optimization**|Peiji Li et.al.|[2601.16480](http://arxiv.org/abs/2601.16480)|null|\n", "2601.16400": "|**2026-01-23**|**Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification**|Zongwan Cao et.al.|[2601.16400](http://arxiv.org/abs/2601.16400)|null|\n", "2601.18533": "|**2026-01-26**|**From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation**|Yuxin Jiang et.al.|[2601.18533](http://arxiv.org/abs/2601.18533)|null|\n", "2601.18292": "|**2026-01-26**|**TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment**|Zhewen Tan et.al.|[2601.18292](http://arxiv.org/abs/2601.18292)|null|\n", "2601.18225": "|**2026-01-26**|**ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants**|Pei Wang et.al.|[2601.18225](http://arxiv.org/abs/2601.18225)|null|\n", "2601.18207": "|**2026-01-26**|**PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR**|James Burgess et.al.|[2601.18207](http://arxiv.org/abs/2601.18207)|null|\n", "2601.17275": "|**2026-01-24**|**Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning**|Lianlei Shan et.al.|[2601.17275](http://arxiv.org/abs/2601.17275)|null|\n", "2601.17223": "|**2026-01-23**|**Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning**|Massimiliano Pronesti et.al.|[2601.17223](http://arxiv.org/abs/2601.17223)|null|\n", "2601.18150": "|**2026-01-26**|**FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning**|Zhaopeng Qiu et.al.|[2601.18150](http://arxiv.org/abs/2601.18150)|null|\n", "2601.20649": "|**2026-01-28**|**P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering**|Wenlin Zhong et.al.|[2601.20649](http://arxiv.org/abs/2601.20649)|null|\n", "2601.20614": "|**2026-01-28**|**Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation**|Yanqi Dai et.al.|[2601.20614](http://arxiv.org/abs/2601.20614)|null|\n", "2601.20305": "|**2026-01-28**|**Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models**|Zhenchen Tang et.al.|[2601.20305](http://arxiv.org/abs/2601.20305)|null|\n", "2601.20126": "|**2026-01-27**|**Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models**|Abha Jha et.al.|[2601.20126](http://arxiv.org/abs/2601.20126)|null|\n", "2601.19280": "|**2026-01-27**|**Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning**|Kishan Panaganti et.al.|[2601.19280](http://arxiv.org/abs/2601.19280)|null|\n", "2601.19620": "|**2026-01-28**|**R^3: Replay, Reflection, and Ranking Rewards for LLM Reinforcement Learning**|Zhizheng Jiang et.al.|[2601.19620](http://arxiv.org/abs/2601.19620)|null|\n", "2601.21459": "|**2026-02-02**|**HER: Human-like Reasoning and Reinforcement Learning for LLM Role-playing**|Chengyu Du et.al.|[2601.21459](http://arxiv.org/abs/2601.21459)|null|\n", "2601.21244": "|**2026-02-02**|**Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification**|Yiju Guo et.al.|[2601.21244](http://arxiv.org/abs/2601.21244)|null|\n", "2601.21192": "|**2026-01-29**|**Do Reasoning Models Enhance Embedding Models?**|Wun Yu Chan et.al.|[2601.21192](http://arxiv.org/abs/2601.21192)|null|\n", "2601.21051": "|**2026-01-28**|**Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report**|Zhuoran Yang et.al.|[2601.21051](http://arxiv.org/abs/2601.21051)|null|\n", "2601.21008": "|**2026-01-28**|**Solver-in-the-Loop: MDP-Based Benchmarks for Self-Correction and Behavioral Rationality in Operations Research**|Ruicheng Ao et.al.|[2601.21008](http://arxiv.org/abs/2601.21008)|null|\n", "2601.20829": "|**2026-01-28**|**Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning**|Minwu Kim et.al.|[2601.20829](http://arxiv.org/abs/2601.20829)|null|\n", "2601.20802": "|**2026-01-28**|**Reinforcement Learning via Self-Distillation**|Jonas H\u00fcbotter et.al.|[2601.20802](http://arxiv.org/abs/2601.20802)|null|\n", "2601.21617": "|**2026-01-29**|**PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization**|Songhan Jiang et.al.|[2601.21617](http://arxiv.org/abs/2601.21617)|null|\n", "2601.21872": "|**2026-01-29**|**WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents**|Yao Zhang et.al.|[2601.21872](http://arxiv.org/abs/2601.21872)|null|\n", "2601.22975": "|**2026-01-30**|**Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text**|Ximing Lu et.al.|[2601.22975](http://arxiv.org/abs/2601.22975)|null|\n", "2601.22900": "|**2026-01-30**|**MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop**|Xuancheng Li et.al.|[2601.22900](http://arxiv.org/abs/2601.22900)|null|\n", "2601.22595": "|**2026-01-30**|**Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR**|Hao Yi et.al.|[2601.22595](http://arxiv.org/abs/2601.22595)|null|\n", "2601.22448": "|**2026-01-30**|**HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning**|Weiqi Wang et.al.|[2601.22448](http://arxiv.org/abs/2601.22448)|null|\n", "2601.22297": "|**2026-01-29**|**Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning**|Chenxi Liu et.al.|[2601.22297](http://arxiv.org/abs/2601.22297)|null|\n", "2602.00815": "|**2026-01-31**|**Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement**|Yunjian Zhang et.al.|[2602.00815](http://arxiv.org/abs/2602.00815)|null|\n", "2602.00759": "|**2026-01-31**|**Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning**|Zhipeng Chen et.al.|[2602.00759](http://arxiv.org/abs/2602.00759)|null|\n", "2602.00632": "|**2026-01-31**|**Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation**|Hongxun Ding et.al.|[2602.00632](http://arxiv.org/abs/2602.00632)|null|\n", "2602.00575": "|**2026-01-31**|**Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction**|Chaoqun Cui et.al.|[2602.00575](http://arxiv.org/abs/2602.00575)|null|\n", "2602.00513": "|**2026-01-31**|**Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs**|Md Tanvirul Alam et.al.|[2602.00513](http://arxiv.org/abs/2602.00513)|null|\n", "2602.00426": "|**2026-01-31**|**LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference**|Vikram Krishnamurthy et.al.|[2602.00426](http://arxiv.org/abs/2602.00426)|null|\n", "2602.00350": "|**2026-01-30**|**ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models**|Ignacy Kolton et.al.|[2602.00350](http://arxiv.org/abs/2602.00350)|null|\n", "2602.00173": "|**2026-01-30**|**Learning Robust Reasoning through Guided Adversarial Self-Play**|Shuozhe Li et.al.|[2602.00173](http://arxiv.org/abs/2602.00173)|null|\n", "2602.00528": "|**2026-01-31**|**How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use**|Minhua Lin et.al.|[2602.00528](http://arxiv.org/abs/2602.00528)|null|\n", "2602.00400": "|**2026-01-30**|**KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning**|Fan Yang et.al.|[2602.00400](http://arxiv.org/abs/2602.00400)|null|\n", "2601.23143": "|**2026-01-30**|**THINKSAFE: Self-Generated Safety Alignment for Reasoning Models**|Seanie Lee et.al.|[2601.23143](http://arxiv.org/abs/2601.23143)|null|\n", "2601.23135": "|**2026-01-30**|**Why GRPO Needs Normalization: A Local-Curvature Perspective on Adaptive Gradients**|Cheng Ge et.al.|[2601.23135](http://arxiv.org/abs/2601.23135)|null|\n", "2602.03452": "|**2026-02-03**|**Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing**|Xin Sheng et.al.|[2602.03452](http://arxiv.org/abs/2602.03452)|null|\n", "2602.03320": "|**2026-02-03**|**MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning**|Shengyuan Liu et.al.|[2602.03320](http://arxiv.org/abs/2602.03320)|null|\n", "2602.03048": "|**2026-02-04**|**CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs**|Zhiyuan Yao et.al.|[2602.03048](http://arxiv.org/abs/2602.03048)|null|\n", "2602.02482": "|**2026-02-02**|**Expanding the Capabilities of Reinforcement Learning via Text Feedback**|Yuda Song et.al.|[2602.02482](http://arxiv.org/abs/2602.02482)|null|\n", "2602.02377": "|**2026-02-02**|**Proof-RM: A Scalable and Generalizable Reward Model for Math Proof**|Haotong Yang et.al.|[2602.02377](http://arxiv.org/abs/2602.02377)|null|\n", "2602.02099": "|**2026-02-02**|**Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning**|Keqin Peng et.al.|[2602.02099](http://arxiv.org/abs/2602.02099)|null|\n", "2602.01791": "|**2026-02-02**|**Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning**|Zheng Zhang et.al.|[2602.01791](http://arxiv.org/abs/2602.01791)|null|\n", "2602.01599": "|**2026-02-02**|**The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR**|Israel Adewuyi et.al.|[2602.01599](http://arxiv.org/abs/2602.01599)|null|\n", "2602.01103": "|**2026-02-01**|**Probing RLVR training instability through the lens of objective-level hacking**|Yiming Dong et.al.|[2602.01103](http://arxiv.org/abs/2602.01103)|null|\n", "2602.03412": "|**2026-02-03**|**Verified Critical Step Optimization for LLM Agents**|Mukai Li et.al.|[2602.03412](http://arxiv.org/abs/2602.03412)|null|\n", "2602.02979": "|**2026-02-03**|**CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning**|Ran Li et.al.|[2602.02979](http://arxiv.org/abs/2602.02979)|null|\n", "2602.01705": "|**2026-02-03**|**Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner**|Haoqiang Kang et.al.|[2602.01705](http://arxiv.org/abs/2602.01705)|null|\n", "2602.01365": "|**2026-02-01**|**When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning**|Wang Yang et.al.|[2602.01365](http://arxiv.org/abs/2602.01365)|null|\n", "2602.04620": "|**2026-02-04**|**QUATRO: Query-Adaptive Trust Region Policy Optimization for LLM Fine-tuning**|Doyeon Lee et.al.|[2602.04620](http://arxiv.org/abs/2602.04620)|null|\n", "2602.04417": "|**2026-02-04**|**EMA Policy Gradient: Taming Reinforcement Learning for LLMs with EMA Anchor and Top-k KL**|Lunjun Zhang et.al.|[2602.04417](http://arxiv.org/abs/2602.04417)|null|\n", "2602.04278": "|**2026-02-04**|**MiniRec: Data-Efficient Reinforcement Learning for LLM-based Recommendation**|Lin Wang et.al.|[2602.04278](http://arxiv.org/abs/2602.04278)|null|\n", "2602.04265": "|**2026-02-04**|**Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning**|Wenze Lin et.al.|[2602.04265](http://arxiv.org/abs/2602.04265)|null|\n", "2602.03978": "|**2026-02-03**|**Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning**|Zidi Xiong et.al.|[2602.03978](http://arxiv.org/abs/2602.03978)|null|\n", "2602.03507": "|**2026-02-03**|**Learning to Reason Faithfully through Step-Level Faithfulness Maximization**|Runquan Gui et.al.|[2602.03507](http://arxiv.org/abs/2602.03507)|null|\n", "2602.04112": "|**2026-02-04**|**DELTA: Deliberative Multi-Agent Reasoning with Reinforcement Learning for Multimodal Psychological Counseling**|Jiangnan Yang et.al.|[2602.04112](http://arxiv.org/abs/2602.04112)|null|\n", "2602.05717": "|**2026-02-05**|**Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification**|Tianyi Wang et.al.|[2602.05717](http://arxiv.org/abs/2602.05717)|null|\n", "2602.05630": "|**2026-02-05**|**Rewards as Labels: Revisiting RLVR from a Classification Perspective**|Zepeng Zhai et.al.|[2602.05630](http://arxiv.org/abs/2602.05630)|null|\n", "2602.05548": "|**2026-02-05**|**Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation**|Zhiqi Yu et.al.|[2602.05548](http://arxiv.org/abs/2602.05548)|null|\n", "2602.05494": "|**2026-02-05**|**A Unified Framework for Rethinking Policy Divergence Measures in GRPO**|Qingyuan Wu et.al.|[2602.05494](http://arxiv.org/abs/2602.05494)|null|\n", "2602.05281": "|**2026-02-05**|**Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities**|Pengyi Li et.al.|[2602.05281](http://arxiv.org/abs/2602.05281)|null|\n", "2602.05261": "|**2026-02-05**|**Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR**|Fanfan Liu et.al.|[2602.05261](http://arxiv.org/abs/2602.05261)|null|\n", "2602.05165": "|**2026-02-05**|**EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization**|Kevin Han et.al.|[2602.05165](http://arxiv.org/abs/2602.05165)|null|\n", "2602.04879": "|**2026-02-04**|**Rethinking the Trust Region in LLM Reinforcement Learning**|Penghui Qi et.al.|[2602.04879](http://arxiv.org/abs/2602.04879)|null|\n", "2602.05758": "|**2026-02-05**|**LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards**|Bowen Ping et.al.|[2602.05758](http://arxiv.org/abs/2602.05758)|null|\n", "2602.05079": "|**2026-02-04**|**Reinforcement Learning Enhancement Using Vector Semantic Representation and Symbolic Reasoning for Human-Centered Autonomous Emergency Braking**|Vinal Asodia et.al.|[2602.05079](http://arxiv.org/abs/2602.05079)|null|\n"}, "Reward & RLVR": {"2512.16918": "|**2025-12-19**|**AdaTooler-V: Adaptive Tool-Use for Images and Videos**|Chaoyang Wang et.al.|[2512.16918](http://arxiv.org/abs/2512.16918)|null|\n", "2512.16912": "|**2025-12-21**|**Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward**|Peter Chen et.al.|[2512.16912](http://arxiv.org/abs/2512.16912)|null|\n", "2512.15274": "|**2025-12-17**|**Well Begun, Half Done: Reinforcement Learning with Prefix Optimization for LLM Reasoning**|Yiliu Sun et.al.|[2512.15274](http://arxiv.org/abs/2512.15274)|null|\n", "2512.15146": "|**2025-12-18**|**Beyond Majority Voting: Towards Fine-grained and More Reliable Reward Signal for Test-Time Reinforcement Learning**|Weiqin Wang et.al.|[2512.15146](http://arxiv.org/abs/2512.15146)|null|\n", "2512.15000": "|**2025-12-17**|**DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding**|Ruiyi Zhang et.al.|[2512.15000](http://arxiv.org/abs/2512.15000)|null|\n", "2512.14944": "|**2025-12-16**|**Puzzle Curriculum GRPO for Vision-Centric Reasoning**|Ahmadreza Jeddi et.al.|[2512.14944](http://arxiv.org/abs/2512.14944)|null|\n", "2512.14698": "|**2025-12-16**|**TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs**|Jun Zhang et.al.|[2512.14698](http://arxiv.org/abs/2512.14698)|null|\n", "2512.13668": "|**2025-12-15**|**A Scientific Reasoning Model for Organic Synthesis Procedure Generation**|Guoqing Liu et.al.|[2512.13668](http://arxiv.org/abs/2512.13668)|null|\n", "2512.13106": "|**2025-12-15**|**TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning**|Shenzhi Yang et.al.|[2512.13106](http://arxiv.org/abs/2512.13106)|null|\n", "2512.12576": "|**2025-12-14**|**Coupled Variational Reinforcement Learning for Language Model General Reasoning**|Xueru Wen et.al.|[2512.12576](http://arxiv.org/abs/2512.12576)|null|\n", "2512.12487": "|**2025-12-13**|**More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models**|Hoang Anh Just et.al.|[2512.12487](http://arxiv.org/abs/2512.12487)|null|\n", "2512.10756": "|**2025-12-11**|**OPV: Outcome-based Process Verifier for Efficient Long Chain-of-Thought Verification**|Zijian Wu et.al.|[2512.10756](http://arxiv.org/abs/2512.10756)|null|\n", "2512.10739": "|**2025-12-12**|**Long-horizon Reasoning Agent for Olympiad-Level Mathematical Problem Solving**|Songyang Gao et.al.|[2512.10739](http://arxiv.org/abs/2512.10739)|null|\n", "2512.09675": "|**2025-12-10**|**d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models**|Leyi Pan et.al.|[2512.09675](http://arxiv.org/abs/2512.09675)|null|\n", "2512.07761": "|**2025-12-08**|**RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models**|Xiqiao Xiong et.al.|[2512.07761](http://arxiv.org/abs/2512.07761)|null|\n", "2510.17057": "|**2025-10-20**|**The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs**|Nikolaus Howe et.al.|[2510.17057](http://arxiv.org/abs/2510.17057)|null|\n", "2509.06024": "|**2025-09-07**|**Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL**|Haoyang He et.al.|[2509.06024](http://arxiv.org/abs/2509.06024)|null|\n", "2508.14153": "|**2025-11-18**|**LENS: Learning to Segment Anything with Unified Reinforced Reasoning**|Lianghui Zhu et.al.|[2508.14153](http://arxiv.org/abs/2508.14153)|null|\n", "2508.12604": "|**2025-08-18**|**SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression**|Yuyang Xu et.al.|[2508.12604](http://arxiv.org/abs/2508.12604)|null|\n", "2508.06125": "|**2025-08-08**|**SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning**|Lin Zhang et.al.|[2508.06125](http://arxiv.org/abs/2508.06125)|null|\n", "2508.01119": "|**2025-11-18**|**The Promise of RL for Autoregressive Image Editing**|Saba Ahmadi et.al.|[2508.01119](http://arxiv.org/abs/2508.01119)|null|\n", "2505.23678": "|**2025-10-20**|**Grounded Reinforcement Learning for Visual Reasoning**|Gabriel Sarch et.al.|[2505.23678](http://arxiv.org/abs/2505.23678)|null|\n", "2502.12853": "|**2025-02-18**|**S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning**|Ruotian Ma et.al.|[2502.12853](http://arxiv.org/abs/2502.12853)|null|\n", "2501.19278": "|**2025-01-31**|**Pheromone-based Learning of Optimal Reasoning Paths**|Anirudh Chari et.al.|[2501.19278](http://arxiv.org/abs/2501.19278)|null|\n", "2412.14405": "|**2025-09-19**|**RaCT: Ranking-aware Chain-of-Thought Optimization for LLMs**|Haowei Liu et.al.|[2412.14405](http://arxiv.org/abs/2412.14405)|null|\n", "2409.12917": "|**2024-10-04**|**Training Language Models to Self-Correct via Reinforcement Learning**|Aviral Kumar et.al.|[2409.12917](http://arxiv.org/abs/2409.12917)|null|\n", "2505.24500": "|**2025-05-30**|**TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence**|Guiyang Hou et.al.|[2505.24500](http://arxiv.org/abs/2505.24500)|null|\n", "2512.17267": "|**2025-12-19**|**AutoMetrics: Approximate Human Judgements with Automatically Generated Evaluators**|Michael J. Ryan et.al.|[2512.17267](http://arxiv.org/abs/2512.17267)|null|\n", "2512.19554": "|**2025-12-22**|**CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal**|Yongxin Wang et.al.|[2512.19554](http://arxiv.org/abs/2512.19554)|null|\n", "2512.19126": "|**2025-12-23**|**AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards**|Zihan Lin et.al.|[2512.19126](http://arxiv.org/abs/2512.19126)|null|\n", "2512.18857": "|**2025-12-21**|**CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning**|Zijun Gao et.al.|[2512.18857](http://arxiv.org/abs/2512.18857)|null|\n", "2512.18730": "|**2025-12-21**|**A Theoretical Lens for RL-Tuned Language Models via Energy-Based Models**|Zhiquan Tan et.al.|[2512.18730](http://arxiv.org/abs/2512.18730)|null|\n", "2512.18215": "|**2025-12-20**|**Stable and Efficient Single-Rollout RL for Multimodal Reasoning**|Rui Liu et.al.|[2512.18215](http://arxiv.org/abs/2512.18215)|null|\n", "2512.20061": "|**2025-12-23**|**Scaling Reinforcement Learning for Content Moderation with Large Language Models**|Hamed Firooz et.al.|[2512.20061](http://arxiv.org/abs/2512.20061)|null|\n", "2512.20884": "|**2025-12-24**|**The Silent Scholar Problem: A Probabilistic Framework for Breaking Epistemic Asymmetry in LLM Agents**|Zan-Kai Chong et.al.|[2512.20884](http://arxiv.org/abs/2512.20884)|null|\n", "2512.20760": "|**2025-12-23**|**Generalization of RLVR Using Causal Reasoning as a Testbed**|Brian Lu et.al.|[2512.20760](http://arxiv.org/abs/2512.20760)|null|\n", "2512.21625": "|**2025-12-25**|**Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards**|Xinyu Tang et.al.|[2512.21625](http://arxiv.org/abs/2512.21625)|null|\n", "2512.21446": "|**2025-12-24**|**dUltra: Ultra-Fast Diffusion Language Models via Reinforcement Learning**|Shirui Chen et.al.|[2512.21446](http://arxiv.org/abs/2512.21446)|null|\n", "2512.23703": "|**2025-12-29**|**Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation**|Huajie Tan et.al.|[2512.23703](http://arxiv.org/abs/2512.23703)|null|\n", "2512.23165": "|**2025-12-30**|**Evaluating Parameter Efficient Methods for RLVR**|Qingyu Yin et.al.|[2512.23165](http://arxiv.org/abs/2512.23165)|null|\n", "2512.22650": "|**2025-12-27**|**Scaling Unverifiable Rewards: A Case Study on Visual Insights**|Shuyu Gan et.al.|[2512.22650](http://arxiv.org/abs/2512.22650)|null|\n", "2512.25063": "|**2025-12-31**|**Many Minds from One Model: Bayesian Transformers for Population Intelligence**|Diji Yang et.al.|[2512.25063](http://arxiv.org/abs/2512.25063)|null|\n", "2512.23760": "|**2025-12-28**|**Audited Skill-Graph Self-Improvement for Agentic LLMs via Verifiable Rewards, Experience Synthesis, and Continual Memory**|Ken Huang et.al.|[2512.23760](http://arxiv.org/abs/2512.23760)|null|\n", "2601.03205": "|**2026-01-06**|**UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward**|Yile Liu et.al.|[2601.03205](http://arxiv.org/abs/2601.03205)|null|\n", "2601.03969": "|**2026-01-07**|**Anti-Length Shift: Dynamic Outlier Truncation for Training Efficient Reasoning Models**|Wei Wu et.al.|[2601.03969](http://arxiv.org/abs/2601.03969)|null|\n", "2601.03948": "|**2026-01-08**|**Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification**|Rui Sun et.al.|[2601.03948](http://arxiv.org/abs/2601.03948)|null|\n", "2601.03823": "|**2026-01-07**|**Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning**|Fei Wu et.al.|[2601.03823](http://arxiv.org/abs/2601.03823)|null|\n", "2601.03723": "|**2026-01-07**|**ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization**|Shijie Zhang et.al.|[2601.03723](http://arxiv.org/abs/2601.03723)|null|\n", "2601.03525": "|**2026-01-07**|**VeRPO: Verifiable Dense Reward Policy Optimization for Code Generation**|Longwen Wang et.al.|[2601.03525](http://arxiv.org/abs/2601.03525)|null|\n", "2601.05175": "|**2026-01-08**|**VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice**|Shuming Liu et.al.|[2601.05175](http://arxiv.org/abs/2601.05175)|null|\n", "2601.05073": "|**2026-01-08**|**Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward**|Jianlong Chen et.al.|[2601.05073](http://arxiv.org/abs/2601.05073)|null|\n", "2601.05053": "|**2026-01-08**|**Reinforced Efficient Reasoning via Semantically Diverse Exploration**|Ziqi Zhao et.al.|[2601.05053](http://arxiv.org/abs/2601.05053)|null|\n", "2601.04954": "|**2026-01-08**|**Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following**|Yirong Zeng et.al.|[2601.04954](http://arxiv.org/abs/2601.04954)|null|\n", "2601.04731": "|**2026-01-08**|**Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models**|Shuyang Jiang et.al.|[2601.04731](http://arxiv.org/abs/2601.04731)|null|\n", "2601.04700": "|**2026-01-08**|**PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards**|Mukesh Ghimire et.al.|[2601.04700](http://arxiv.org/abs/2601.04700)|null|\n", "2601.04674": "|**2026-01-08**|**PROMISE: Process Reward Models Unlock Test-Time Scaling Laws in Generative Recommendations**|Chengcheng Guo et.al.|[2601.04674](http://arxiv.org/abs/2601.04674)|null|\n", "2601.04611": "|**2026-01-08**|**Character-R1: Enhancing Role-Aware Reasoning in Role-Playing Agents via RLVR**|Yihong Tang et.al.|[2601.04611](http://arxiv.org/abs/2601.04611)|null|\n", "2601.04537": "|**2026-01-08**|**Not All Steps are Informative: On the Linearity of LLMs' RLVR Training**|Tianle Wang et.al.|[2601.04537](http://arxiv.org/abs/2601.04537)|null|\n", "2601.04411": "|**2026-01-07**|**Rate or Fate? RLV$^\\varepsilon$R: Reinforcement Learning with Verifiable Noisy Rewards**|Ali Rad et.al.|[2601.04411](http://arxiv.org/abs/2601.04411)|null|\n", "2601.06021": "|**2026-01-09**|**Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards**|Jiajie Zhang et.al.|[2601.06021](http://arxiv.org/abs/2601.06021)|null|\n", "2601.05870": "|**2026-01-09**|**IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck**|Huilin Deng et.al.|[2601.05870](http://arxiv.org/abs/2601.05870)|null|\n", "2601.05787": "|**2026-01-09**|**From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation**|Zezhou Wang et.al.|[2601.05787](http://arxiv.org/abs/2601.05787)|null|\n", "2601.05688": "|**2026-01-09**|**SketchVL: Policy Optimization via Fine-Grained Credit Assignment for Chart Understanding and More**|Muye Huang et.al.|[2601.05688](http://arxiv.org/abs/2601.05688)|null|\n", "2601.05607": "|**2026-01-09**|**Orchestrating Tokens and Sequences: Dynamic Hybrid Policy Optimization for RLVR**|Zijun Min et.al.|[2601.05607](http://arxiv.org/abs/2601.05607)|null|\n", "2601.07782": "|**2026-01-12**|**Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning**|Wei Fang et.al.|[2601.07782](http://arxiv.org/abs/2601.07782)|null|\n", "2601.07695": "|**2026-01-15**|**Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model**|Siwen Jiao et.al.|[2601.07695](http://arxiv.org/abs/2601.07695)|null|\n", "2601.07349": "|**2026-01-12**|**Reward Modeling from Natural Language Human Feedback**|Zongqi Wang et.al.|[2601.07349](http://arxiv.org/abs/2601.07349)|null|\n", "2601.07320": "|**2026-01-12**|**Segmental Advantage Estimation: Enhancing PPO for Long-Context LLM Training**|Xue Gong et.al.|[2601.07320](http://arxiv.org/abs/2601.07320)|null|\n", "2601.07280": "|**2026-01-12**|**ReasonTabQA: A Comprehensive Benchmark for Table Question Answering from Real World Industrial Scenarios**|Changzai Pan et.al.|[2601.07280](http://arxiv.org/abs/2601.07280)|null|\n", "2601.07226": "|**2026-01-12**|**Lost in the Noise: How Reasoning Models Fail with Contextual Distractors**|Seongyun Lee et.al.|[2601.07226](http://arxiv.org/abs/2601.07226)|null|\n", "2601.07182": "|**2026-01-13**|**PRPO: Aligning Process Reward with Outcome Reward in Policy Optimization**|Ruiyi Ding et.al.|[2601.07182](http://arxiv.org/abs/2601.07182)|null|\n", "2601.06801": "|**2026-01-11**|**Thinking with Deltas: Incentivizing Reinforcement Learning via Differential Visual Reasoning Policy**|Shujian Gao et.al.|[2601.06801](http://arxiv.org/abs/2601.06801)|null|\n", "2601.06794": "|**2026-01-11**|**No More Stale Feedback: Co-Evolving Critics for Open-World Agent Learning**|Zhicong Li et.al.|[2601.06794](http://arxiv.org/abs/2601.06794)|null|\n", "2601.06767": "|**2026-01-11**|**GanitLLM: Difficulty-Aware Bengali Mathematical Reasoning through Curriculum-GRPO**|Shubhashis Roy Dipta et.al.|[2601.06767](http://arxiv.org/abs/2601.06767)|null|\n", "2601.06677": "|**2026-01-10**|**Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget**|Zohaib Khan et.al.|[2601.06677](http://arxiv.org/abs/2601.06677)|null|\n", "2601.06336": "|**2026-01-09**|**Future-as-Label: Scalable Supervision from Real-World Outcomes**|Benjamin Turtel et.al.|[2601.06336](http://arxiv.org/abs/2601.06336)|null|\n", "2601.08521": "|**2026-01-13**|**Your Group-Relative Advantage Is Biased**|Fengkai Yang et.al.|[2601.08521](http://arxiv.org/abs/2601.08521)|null|\n", "2601.08468": "|**2026-01-13**|**JudgeRLVR: Judge First, Generate Second for Efficient Reasoning**|Jiangshan Duo et.al.|[2601.08468](http://arxiv.org/abs/2601.08468)|null|\n", "2601.08430": "|**2026-01-13**|**RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation**|Sunzhu Li et.al.|[2601.08430](http://arxiv.org/abs/2601.08430)|null|\n", "2601.08237": "|**2026-01-13**|**The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination**|Haoran Su et.al.|[2601.08237](http://arxiv.org/abs/2601.08237)|null|\n", "2601.09361": "|**2026-01-14**|**GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR**|Jiaying Zhang et.al.|[2601.09361](http://arxiv.org/abs/2601.09361)|null|\n", "2601.10306": "|**2026-01-15**|**Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning**|Xin Guan et.al.|[2601.10306](http://arxiv.org/abs/2601.10306)|null|\n", "2601.10245": "|**2026-01-15**|**TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks**|Vansh Kapoor et.al.|[2601.10245](http://arxiv.org/abs/2601.10245)|null|\n", "2601.10201": "|**2026-01-15**|**PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary**|Jiarui Yao et.al.|[2601.10201](http://arxiv.org/abs/2601.10201)|null|\n", "2601.11061": "|**2026-01-16**|**Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs**|Lecheng Yan et.al.|[2601.11061](http://arxiv.org/abs/2601.11061)|null|\n", "2601.14209": "|**2026-01-20**|**InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning**|Matthew Y. R. Yang et.al.|[2601.14209](http://arxiv.org/abs/2601.14209)|null|\n", "2601.13284": "|**2026-01-19**|**Balancing Classification and Calibration Performance in Decision-Making LLMs via Calibration Aware Reinforcement Learning**|Duygu Nur Yaldiz et.al.|[2601.13284](http://arxiv.org/abs/2601.13284)|null|\n", "2601.12995": "|**2026-01-19**|**Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models**|Runxuan Liu et.al.|[2601.12995](http://arxiv.org/abs/2601.12995)|null|\n", "2601.12748": "|**2026-01-19**|**Towards Robust Process Reward Modeling via Noise-aware Learning**|Bin Xie et.al.|[2601.12748](http://arxiv.org/abs/2601.12748)|null|\n", "2601.12465": "|**2026-01-18**|**Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping**|Miao Peng et.al.|[2601.12465](http://arxiv.org/abs/2601.12465)|null|\n", "2601.12294": "|**2026-01-18**|**ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents**|Dawei Li et.al.|[2601.12294](http://arxiv.org/abs/2601.12294)|null|\n", "2601.12186": "|**2026-01-17**|**Aletheia: What Makes RLVR For Code Verifiers Tick?**|Vatsal Venkatkrishna et.al.|[2601.12186](http://arxiv.org/abs/2601.12186)|null|\n", "2601.11864": "|**2026-01-17**|**AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language Model Training**|Zhiyuan Li et.al.|[2601.11864](http://arxiv.org/abs/2601.11864)|null|\n", "2601.14700": "|**2026-01-21**|**DARL: Encouraging Diverse Answers for General Reasoning without Verifiers**|Chongxuan Huang et.al.|[2601.14700](http://arxiv.org/abs/2601.14700)|null|\n", "2601.14456": "|**2026-01-20**|**On the Generalization Gap in LLM Planning: Tests and Verifier-Reward RL**|Valerio Belcamino et.al.|[2601.14456](http://arxiv.org/abs/2601.14456)|null|\n", "2601.15808": "|**2026-01-22**|**Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification**|Yuxuan Wan et.al.|[2601.15808](http://arxiv.org/abs/2601.15808)|null|\n", "2601.15737": "|**2026-01-22**|**PhysProver: Advancing Automatic Theorem Proving for Physics**|Hanning Zhang et.al.|[2601.15737](http://arxiv.org/abs/2601.15737)|null|\n", "2601.15668": "|**2026-01-22**|**EmotionThinker: Prosody-Aware Reinforcement Learning for Explainable Speech Emotion Reasoning**|Dingdong Wang et.al.|[2601.15668](http://arxiv.org/abs/2601.15668)|null|\n", "2601.15609": "|**2026-01-26**|**When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in RL with Verifiable Rewards**|Mingyuan Fan et.al.|[2601.15609](http://arxiv.org/abs/2601.15609)|null|\n", "2601.15330": "|**2026-01-20**|**ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn Conversation**|Zhebo Wang et.al.|[2601.15330](http://arxiv.org/abs/2601.15330)|null|\n", "2601.16853": "|**2026-01-23**|**Reasoning Promotes Robustness in Theory of Mind Tasks**|Ian B. de Haan et.al.|[2601.16853](http://arxiv.org/abs/2601.16853)|null|\n", "2601.18533": "|**2026-01-26**|**From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation**|Yuxin Jiang et.al.|[2601.18533](http://arxiv.org/abs/2601.18533)|null|\n", "2601.18207": "|**2026-01-26**|**PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR**|James Burgess et.al.|[2601.18207](http://arxiv.org/abs/2601.18207)|null|\n", "2601.17223": "|**2026-01-23**|**Beyond Outcome Verification: Verifiable Process Reward Models for Structured Reasoning**|Massimiliano Pronesti et.al.|[2601.17223](http://arxiv.org/abs/2601.17223)|null|\n", "2601.20649": "|**2026-01-28**|**P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering**|Wenlin Zhong et.al.|[2601.20649](http://arxiv.org/abs/2601.20649)|null|\n", "2601.20614": "|**2026-01-28**|**Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation**|Yanqi Dai et.al.|[2601.20614](http://arxiv.org/abs/2601.20614)|null|\n", "2601.20585": "|**2026-01-28**|**Ranking-aware Reinforcement Learning for Ordinal Ranking**|Aiming Hao et.al.|[2601.20585](http://arxiv.org/abs/2601.20585)|null|\n", "2601.20312": "|**2026-01-28**|**SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger**|Kaiyuan Chen et.al.|[2601.20312](http://arxiv.org/abs/2601.20312)|null|\n", "2601.20305": "|**2026-01-28**|**Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models**|Zhenchen Tang et.al.|[2601.20305](http://arxiv.org/abs/2601.20305)|null|\n", "2601.20126": "|**2026-01-27**|**Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models**|Abha Jha et.al.|[2601.20126](http://arxiv.org/abs/2601.20126)|null|\n", "2601.18984": "|**2026-01-26**|**Save the Good Prefix: Precise Error Penalization via Process-Supervised RL to Enhance LLM Reasoning**|Haolin Liu et.al.|[2601.18984](http://arxiv.org/abs/2601.18984)|null|\n", "2601.21244": "|**2026-01-29**|**Less Noise, More Voice: Reinforcement Learning for Reasoning via Instruction Purification**|Yiju Guo et.al.|[2601.21244](http://arxiv.org/abs/2601.21244)|null|\n", "2601.21192": "|**2026-01-29**|**Do Reasoning Models Enhance Embedding Models?**|Wun Yu Chan et.al.|[2601.21192](http://arxiv.org/abs/2601.21192)|null|\n", "2601.21051": "|**2026-01-28**|**Llama-3.1-FoundationAI-SecurityLLM-Reasoning-8B Technical Report**|Zhuoran Yang et.al.|[2601.21051](http://arxiv.org/abs/2601.21051)|null|\n", "2601.20829": "|**2026-01-28**|**Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning**|Minwu Kim et.al.|[2601.20829](http://arxiv.org/abs/2601.20829)|null|\n", "2601.20802": "|**2026-01-28**|**Reinforcement Learning via Self-Distillation**|Jonas H\u00fcbotter et.al.|[2601.20802](http://arxiv.org/abs/2601.20802)|null|\n", "2601.21912": "|**2026-01-29**|**ProRAG: Process-Supervised Reinforcement Learning for Retrieval-Augmented Generation**|Zhao Wang et.al.|[2601.21912](http://arxiv.org/abs/2601.21912)|null|\n", "2601.21872": "|**2026-01-29**|**WebArbiter: A Principle-Guided Reasoning Process Reward Model for Web Agents**|Yao Zhang et.al.|[2601.21872](http://arxiv.org/abs/2601.21872)|null|\n", "2601.22975": "|**2026-01-30**|**Golden Goose: A Simple Trick to Synthesize Unlimited RLVR Tasks from Unverifiable Internet Text**|Ximing Lu et.al.|[2601.22975](http://arxiv.org/abs/2601.22975)|null|\n", "2601.22900": "|**2026-01-30**|**MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop**|Xuancheng Li et.al.|[2601.22900](http://arxiv.org/abs/2601.22900)|null|\n", "2601.22776": "|**2026-01-30**|**TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization**|Shichao Ma et.al.|[2601.22776](http://arxiv.org/abs/2601.22776)|null|\n", "2601.22607": "|**2026-01-30**|**From Self-Evolving Synthetic Data to Verifiable-Reward RL: Post-Training Multi-turn Interactive Tool-Using Agents**|Jiaxuan Gao et.al.|[2601.22607](http://arxiv.org/abs/2601.22607)|null|\n", "2601.22595": "|**2026-01-30**|**Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR**|Hao Yi et.al.|[2601.22595](http://arxiv.org/abs/2601.22595)|null|\n", "2601.22532": "|**2026-01-30**|**Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective**|Hong Xie et.al.|[2601.22532](http://arxiv.org/abs/2601.22532)|null|\n", "2601.22530": "|**2026-01-30**|**Enhancing TableQA through Verifiable Reasoning Trace Reward**|Tung Sum Thomas Kwok et.al.|[2601.22530](http://arxiv.org/abs/2601.22530)|null|\n", "2601.22491": "|**2026-01-30**|**SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization**|Jinyang Wu et.al.|[2601.22491](http://arxiv.org/abs/2601.22491)|null|\n", "2601.22297": "|**2026-01-29**|**Prepare Reasoning Language Models for Multi-Agent Debate with Self-Debate Reinforcement Learning**|Chenxi Liu et.al.|[2601.22297](http://arxiv.org/abs/2601.22297)|null|\n", "2601.22249": "|**2026-01-29**|**FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation**|Ruiyi Zhang et.al.|[2601.22249](http://arxiv.org/abs/2601.22249)|null|\n", "2601.22230": "|**2026-01-29**|**DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation**|Peijia Qin et.al.|[2601.22230](http://arxiv.org/abs/2601.22230)|null|\n", "2602.00983": "|**2026-02-01**|**DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning**|Batuhan K. Karaman et.al.|[2602.00983](http://arxiv.org/abs/2602.00983)|null|\n", "2602.00815": "|**2026-01-31**|**Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement**|Yunjian Zhang et.al.|[2602.00815](http://arxiv.org/abs/2602.00815)|null|\n", "2602.00759": "|**2026-01-31**|**Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning**|Zhipeng Chen et.al.|[2602.00759](http://arxiv.org/abs/2602.00759)|null|\n", "2602.00575": "|**2026-01-31**|**Agentic Reward Modeling: Verifying GUI Agent via Online Proactive Interaction**|Chaoqun Cui et.al.|[2602.00575](http://arxiv.org/abs/2602.00575)|null|\n", "2602.00564": "|**2026-01-31**|**Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs**|Xiang Zheng et.al.|[2602.00564](http://arxiv.org/abs/2602.00564)|null|\n", "2602.00513": "|**2026-01-31**|**Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs**|Md Tanvirul Alam et.al.|[2602.00513](http://arxiv.org/abs/2602.00513)|null|\n", "2602.00426": "|**2026-01-31**|**LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference**|Vikram Krishnamurthy et.al.|[2602.00426](http://arxiv.org/abs/2602.00426)|null|\n", "2602.00350": "|**2026-01-30**|**ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models**|Ignacy Kolton et.al.|[2602.00350](http://arxiv.org/abs/2602.00350)|null|\n", "2602.03452": "|**2026-02-03**|**Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing**|Xin Sheng et.al.|[2602.03452](http://arxiv.org/abs/2602.03452)|null|\n", "2602.03412": "|**2026-02-03**|**Verified Critical Step Optimization for LLM Agents**|Mukai Li et.al.|[2602.03412](http://arxiv.org/abs/2602.03412)|null|\n", "2602.03320": "|**2026-02-03**|**MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning**|Shengyuan Liu et.al.|[2602.03320](http://arxiv.org/abs/2602.03320)|null|\n", "2602.03143": "|**2026-02-03**|**Self-Hinting Language Models Enhance Reinforcement Learning**|Baohao Liao et.al.|[2602.03143](http://arxiv.org/abs/2602.03143)|null|\n", "2602.03094": "|**2026-02-03**|**Test-time Recursive Thinking: Self-Improvement without External Feedback**|Yufan Zhuang et.al.|[2602.03094](http://arxiv.org/abs/2602.03094)|null|\n", "2602.03053": "|**2026-02-03**|**MAS-ProVe: Understanding the Process Verification of Multi-Agent Systems**|Vishal Venkataramani et.al.|[2602.03053](http://arxiv.org/abs/2602.03053)|null|\n", "2602.03048": "|**2026-02-04**|**CoBA-RL: Capability-Oriented Budget Allocation for Reinforcement Learning in LLMs**|Zhiyuan Yao et.al.|[2602.03048](http://arxiv.org/abs/2602.03048)|null|\n", "2602.02377": "|**2026-02-02**|**Proof-RM: A Scalable and Generalizable Reward Model for Math Proof**|Haotong Yang et.al.|[2602.02377](http://arxiv.org/abs/2602.02377)|null|\n", "2602.02099": "|**2026-02-02**|**Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning**|Keqin Peng et.al.|[2602.02099](http://arxiv.org/abs/2602.02099)|null|\n", "2602.02050": "|**2026-02-02**|**Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents**|Zeping Li et.al.|[2602.02050](http://arxiv.org/abs/2602.02050)|null|\n", "2602.01791": "|**2026-02-02**|**Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning**|Zheng Zhang et.al.|[2602.01791](http://arxiv.org/abs/2602.01791)|null|\n", "2602.01601": "|**2026-02-03**|**Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards**|Hieu Trung Nguyen et.al.|[2602.01601](http://arxiv.org/abs/2602.01601)|null|\n", "2602.01599": "|**2026-02-02**|**The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR**|Israel Adewuyi et.al.|[2602.01599](http://arxiv.org/abs/2602.01599)|null|\n", "2602.01523": "|**2026-02-02**|**A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning**|Akifumi Wachi et.al.|[2602.01523](http://arxiv.org/abs/2602.01523)|null|\n", "2602.01103": "|**2026-02-01**|**Probing RLVR training instability through the lens of objective-level hacking**|Yiming Dong et.al.|[2602.01103](http://arxiv.org/abs/2602.01103)|null|\n", "2602.04265": "|**2026-02-04**|**Thickening-to-Thinning: Reward Shaping via Human-Inspired Learning Dynamics for LLM Reasoning**|Wenze Lin et.al.|[2602.04265](http://arxiv.org/abs/2602.04265)|null|\n", "2602.04145": "|**2026-02-05**|**Training Data Efficiency in Multimodal Process Reward Models**|Jinyuan Li et.al.|[2602.04145](http://arxiv.org/abs/2602.04145)|null|\n", "2602.03978": "|**2026-02-03**|**Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning**|Zidi Xiong et.al.|[2602.03978](http://arxiv.org/abs/2602.03978)|null|\n", "2602.03719": "|**2026-02-03**|**Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling**|Yubao Zhao et.al.|[2602.03719](http://arxiv.org/abs/2602.03719)|null|\n", "2602.03619": "|**2026-02-03**|**Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation**|Changze Lv et.al.|[2602.03619](http://arxiv.org/abs/2602.03619)|null|\n", "2602.03507": "|**2026-02-03**|**Learning to Reason Faithfully through Step-Level Faithfulness Maximization**|Runquan Gui et.al.|[2602.03507](http://arxiv.org/abs/2602.03507)|null|\n", "2602.05717": "|**2026-02-05**|**Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification**|Tianyi Wang et.al.|[2602.05717](http://arxiv.org/abs/2602.05717)|null|\n", "2602.05630": "|**2026-02-05**|**Rewards as Labels: Revisiting RLVR from a Classification Perspective**|Zepeng Zhai et.al.|[2602.05630](http://arxiv.org/abs/2602.05630)|null|\n", "2602.05548": "|**2026-02-05**|**Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation**|Zhiqi Yu et.al.|[2602.05548](http://arxiv.org/abs/2602.05548)|null|\n", "2602.05494": "|**2026-02-05**|**A Unified Framework for Rethinking Policy Divergence Measures in GRPO**|Qingyuan Wu et.al.|[2602.05494](http://arxiv.org/abs/2602.05494)|null|\n", "2602.05281": "|**2026-02-05**|**Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities**|Pengyi Li et.al.|[2602.05281](http://arxiv.org/abs/2602.05281)|null|\n", "2602.05261": "|**2026-02-05**|**Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR**|Fanfan Liu et.al.|[2602.05261](http://arxiv.org/abs/2602.05261)|null|\n", "2602.05165": "|**2026-02-05**|**EBPO: Empirical Bayes Shrinkage for Stabilizing Group-Relative Policy Optimization**|Kevin Han et.al.|[2602.05165](http://arxiv.org/abs/2602.05165)|null|\n", "2602.04928": "|**2026-02-04**|**Euphonium: Steering Video Flow Matching via Process Reward Gradient Guided Stochastic Dynamics**|Ruizhe Zhong et.al.|[2602.04928](http://arxiv.org/abs/2602.04928)|null|\n"}, "RL & LLM for Auto-Driving": {"2512.10719": "|**2025-12-11**|**SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving**|Peizheng Li et.al.|[2512.10719](http://arxiv.org/abs/2512.10719)|null|\n", "2512.04459": "|**2025-12-04**|**dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning**|Yingzi Ma et.al.|[2512.04459](http://arxiv.org/abs/2512.04459)|null|\n", "2512.02966": "|**2025-12-02**|**Lumos: Let there be Language Model System Certification**|Isha Chaudhary et.al.|[2512.02966](http://arxiv.org/abs/2512.02966)|null|\n", "2512.01830": "|**2025-12-02**|**OpenREAD: Reinforced Open-Ended Reasoning for End-to-End Autonomous Driving with LLM-as-Critic**|Songyan Zhang et.al.|[2512.01830](http://arxiv.org/abs/2512.01830)|null|\n", "2512.01300": "|**2025-12-01**|**RoboDriveVLM: A Novel Benchmark and Baseline towards Robust Vision-Language Models for Autonomous Driving**|Dacheng Liao et.al.|[2512.01300](http://arxiv.org/abs/2512.01300)|null|\n", "2511.19914": "|**2025-11-25**|**CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model**|Dapeng Zhang et.al.|[2511.19914](http://arxiv.org/abs/2511.19914)|null|\n", "2511.14592": "|**2025-11-19**|**Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks**|Xianhui Meng et.al.|[2511.14592](http://arxiv.org/abs/2511.14592)|null|\n", "2511.14391": "|**2025-11-18**|**Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition**|Fabian Schmidt et.al.|[2511.14391](http://arxiv.org/abs/2511.14391)|null|\n", "2511.09025": "|**2025-11-12**|**FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks**|Tianao Xiang et.al.|[2511.09025](http://arxiv.org/abs/2511.09025)|null|\n", "2511.06253": "|**2025-11-09**|**AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving**|Ruifei Zhang et.al.|[2511.06253](http://arxiv.org/abs/2511.06253)|null|\n", "2510.24152": "|**2025-10-28**|**Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning**|Aodi Wu et.al.|[2510.24152](http://arxiv.org/abs/2510.24152)|null|\n", "2510.04532": "|**2025-10-06**|**More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models**|Xurui Song et.al.|[2510.04532](http://arxiv.org/abs/2510.04532)|null|\n", "2509.24408": "|**2025-12-29**|**FuncPoison: Poisoning Function Library to Hijack Multi-agent Autonomous Driving Systems**|Yuzhen Long et.al.|[2509.24408](http://arxiv.org/abs/2509.24408)|null|\n", "2508.13305": "|**2025-08-18**|**Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving**|Minhao Xiong et.al.|[2508.13305](http://arxiv.org/abs/2508.13305)|null|\n", "2508.12404": "|**2025-08-17**|**LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving**|Nan Song et.al.|[2508.12404](http://arxiv.org/abs/2508.12404)|null|\n", "2512.11872": "|**2025-12-06**|**WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving**|Mingwang Xu et.al.|[2512.11872](http://arxiv.org/abs/2512.11872)|null|\n", "2511.07155": "|**2025-11-10**|**Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving**|Thomas Steinecker et.al.|[2511.07155](http://arxiv.org/abs/2511.07155)|null|\n", "2509.13769": "|**2025-09-17**|**AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving**|Yuechen Luo et.al.|[2509.13769](http://arxiv.org/abs/2509.13769)|null|\n", "2509.08221": "|**2025-09-10**|**A Comprehensive Review of Reinforcement Learning for Autonomous Driving in the CARLA Simulator**|Elahe Delavari et.al.|[2509.08221](http://arxiv.org/abs/2509.08221)|null|\n", "2508.05167": "|**2025-08-07**|**PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems**|Qi Guo et.al.|[2508.05167](http://arxiv.org/abs/2508.05167)|null|\n", "2507.05251": "|**2025-07-07**|**Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving**|Elahe Delavari et.al.|[2507.05251](http://arxiv.org/abs/2507.05251)|null|\n", "2506.08533": "|**2025-06-10**|**Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)**|Nihal Acharya Adde et.al.|[2506.08533](http://arxiv.org/abs/2506.08533)|null|\n", "2506.06218": "|**2025-06-06**|**STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving**|Christian Fruhwirth-Reisinger et.al.|[2506.06218](http://arxiv.org/abs/2506.06218)|null|\n", "2505.24317": "|**2025-05-30**|**ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving**|Yongming Chen et.al.|[2505.24317](http://arxiv.org/abs/2505.24317)|null|\n", "2505.16377": "|**2025-05-22**|**VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving**|Yansong Qu et.al.|[2505.16377](http://arxiv.org/abs/2505.16377)|null|\n", "2505.08264": "|**2025-07-11**|**Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning**|Ahmed Abouelazm et.al.|[2505.08264](http://arxiv.org/abs/2505.08264)|null|\n", "2505.06737": "|**2025-07-11**|**Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving**|Ahmed Abouelazm et.al.|[2505.06737](http://arxiv.org/abs/2505.06737)|null|\n", "2505.01440": "|**2025-04-28**|**Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving**|Alkis Sygkounas et.al.|[2505.01440](http://arxiv.org/abs/2505.01440)|null|\n", "2503.19690": "|**2025-03-27**|**Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection**|Bo Leng et.al.|[2503.19690](http://arxiv.org/abs/2503.19690)|null|\n", "2503.11400": "|**2025-03-14**|**A Framework for a Capability-driven Evaluation of Scenario Understanding for Multimodal Large Language Models in Autonomous Driving**|Tin Stribor Sohn et.al.|[2503.11400](http://arxiv.org/abs/2503.11400)|null|\n", "2601.05640": "|**2026-01-12**|**SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving**|Jingyu Li et.al.|[2601.05640](http://arxiv.org/abs/2601.05640)|null|\n", "2601.12142": "|**2026-01-29**|**Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving**|Ziang Guo et.al.|[2601.12142](http://arxiv.org/abs/2601.12142)|null|\n", "2601.19582": "|**2026-01-27**|**ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving**|Yujin Wang et.al.|[2601.19582](http://arxiv.org/abs/2601.19582)|null|\n", "2601.21288": "|**2026-01-29**|**Drive-KD: Multi-Teacher Distillation for VLMs in Autonomous Driving**|Weitong Lian et.al.|[2601.21288](http://arxiv.org/abs/2601.21288)|null|\n", "2601.22930": "|**2026-01-30**|**MTDrive: Multi-turn Interactive Reinforcement Learning for Autonomous Driving**|Xidong Li et.al.|[2601.22930](http://arxiv.org/abs/2601.22930)|null|\n"}}